big3 = readRDS("Clean-up/Vector_normalized/vn_all_w.rds")
meta(big3)$Location
unique(meta(big3)$Location)
big3 = spec[meta(spec)$Location == c("Wickersham Dome B", "Wickersham Dome A", "Eagle Summit")]
spec = readRDS("Clean-up/Vector_normalized/all_vn.rds")
big3 = spec[meta(spec)$Location == c("Wickersham Dome B", "Wickersham Dome A", "Eagle Summit")]
big3 = spec[meta(spec)$Location == c("Wickersham Dome B", "Wickersham Dome A", "Eagle Summit"),]
big3 = spec[meta(spec)$Location == "Wickersham Dome B",]
spec = readRDS("Clean-up/Vector_normalized/all_vn.rds")
spec1 = spec[meta(spec)$Location == "Wickersham Dome B",]
spec2 = spec[meta(spec)$Location == "Wickersham Dome A",]
spec3 = spec[meta(spec)$Location == "Eagle Summit",]
big3 = Reduce(combine, list(spec1, spec2, spec3))
unique(meta(big3)$Location)
big3 = Reduce(combine, list(spec1, spec2, spec3))
names(big3) = meta(big3)$Species_ID
big3.m = as.matrix(big3)
big3.df = as.data.frame(big3)
#Resample by 10 nm
spec_small = resample(big3, seq(400, 2400, by = 10))
spec_mat_s = as.matrix(spec_small)
spec_mat = spec_mat_s
resp = rownames(spec_mat)
rownames(spec_mat) = seq(nrow(spec_mat))
#determine number of components to use
plsda.fit = plsda(spec_mat, resp, ncomp = 30)
perf.plsda = perf(plsda.fit, validation = "Mfold", folds = 5,
progressBar = TRUE, auc = TRUE, nrepeat = 10)
plot(perf.plsda, col = color.mixo(1:3), sd = TRUE,
legend.position = "horizontal")
set.seed(27)
samp <- sample(1:3, nrow(spec_mat), replace = TRUE)
# 1/3 of the data will compose the test set
test <- which(samp == 1)
# rest will compose the training set
train <- setdiff(1:nrow(spec_mat), test)
## For PLS-DA, train the model
plsda.train <- plsda(spec_mat[train, ], resp[train], ncomp = 30)
# then predict
test.predict <- predict(plsda.train, spec_mat[test, ], dist = "max.dist")
# store prediction for the 4th component
prediction <- test.predict$class$max.dist[,24]
# calculate the error rate of the model
confusion.mat = get.confusion_matrix(truth = resp[test], predicted = prediction)
cm1 = as.data.frame(confusion.mat)
get.BER(confusion.mat)
#plot
par(mar = c(2, 4, 3, 4), oma = c(2, 4, 3, 2))
color2D.matplot(cm1,
show.values = TRUE,
axes = FALSE,
xlab = "",
ylab = "",
vcex = 2,
vcol = "black",
extremes = c("white", "deepskyblue3"))
axis(3, at = seq_len(ncol(cm1)) - 0.5,
labels = names(cm1), tick = FALSE, cex.axis = 1)
axis(2, at = seq_len(nrow(cm1)) -0.5,
labels = rev(rownames(cm1)), tick = FALSE, las = 1, cex.axis = 1)
set.seed(27)
samp <- sample(1:3, nrow(spec_mat), replace = TRUE)
# 1/3 of the data will compose the test set
test <- which(samp == 1)
# rest will compose the training set
train <- setdiff(1:nrow(spec_mat), test)
## For PLS-DA, train the model
plsda.train <- plsda(spec_mat[train, ], resp[train], ncomp = 30)
# then predict
test.predict <- predict(plsda.train, spec_mat[test, ], dist = "max.dist")
# store prediction for the 4th component
prediction <- test.predict$class$max.dist[,25]
# calculate the error rate of the model
confusion.mat = get.confusion_matrix(truth = resp[test], predicted = prediction)
cm1 = as.data.frame(confusion.mat)
get.BER(confusion.mat)
#plot
par(mar = c(2, 4, 3, 4), oma = c(2, 4, 3, 2))
color2D.matplot(cm1,
show.values = TRUE,
axes = FALSE,
xlab = "",
ylab = "",
vcex = 2,
vcol = "black",
extremes = c("white", "deepskyblue3"))
axis(3, at = seq_len(ncol(cm1)) - 0.5,
labels = names(cm1), tick = FALSE, cex.axis = 1)
axis(2, at = seq_len(nrow(cm1)) -0.5,
labels = rev(rownames(cm1)), tick = FALSE, las = 1, cex.axis = 1)
big3 = readRDS("Clean-up/Vector_normalized/vn_all_w.rds")
names(big3) = meta(big3)$Species_ID
big3.m = as.matrix(big3)
big3.df = as.data.frame(big3)
#Resample by 10 nm
spec_small = resample(big3, seq(400, 2400, by = 10))
spec_mat_s = as.matrix(spec_small)
spec_mat = spec_mat_s
resp = rownames(spec_mat)
rownames(spec_mat) = seq(nrow(spec_mat))
#determine number of components to use
plsda.fit = plsda(spec_mat, resp, ncomp = 30)
perf.plsda = perf(plsda.fit, validation = "Mfold", folds = 5,
progressBar = TRUE, auc = TRUE, nrepeat = 10)
plot(perf.plsda, col = color.mixo(1:3), sd = TRUE,
legend.position = "horizontal")
meta(big3)
################################################################################
#Clean up scans
#Data can be obtained from shared Google Drive folder:
#FM_Polar_Studies > Dryas_Spectral_Analyses > Scans_raw
################################################################################
#Load Packages
################################################################################
library("spectrolab")
################################################################################
#Prerequisite functions
################################################################################
#add metadata to raw spectra
add_meta <- function(spectra_path, metadata_path){
spectra_raw = read_spectra(path = spectra_path, format = "sed")
metadata = read.csv(file = metadata_path, header = TRUE, stringsAsFactors = FALSE)
meta(spectra_raw) <- metadata
return(spectra_raw)
}
#The goal of the following functions is to keep the 4 spectral measurements that
#are closest to the mean reflectance values for each individual plant. The
#sample design involved stacking leaves 3 times and taking 2 reflectance
#measurements per stack interval. Therefore, the first stack tended to have much
#of the black background showing, and the third stack probably had more light
#reflected from the leaves than observed in nature (think about how stacking a
#leaf would simulate scanning a thicker leaf). The idea here is that restricting
#measurements to the 3 closest to the mean should provide more data to work with
#compared to just taking the mean while also removing high and low reflectance
#measurements caused by sample design.
#subtract the mean reflectance from measured reflectance (i.e. calculate
#distance from the mean)
center_scale = function(spectra){
scale(spectra, scale = FALSE)
}
#rank spectra by distance from the mean
dist.rank = function(spectra){
rank(rowSums(abs(center_scale(spectra))))
}
#keep the 4 spectra per individual plant that are closest to the mean
keep = function(spectra){
a = dist.rank(spectra)
x1 = subset(spectra, a < 5)
}
#split the spectra objects to individual plants, apply above funtions, and
#recombine them.
trim.spectra = function(spectra){
spec.list = lapply(split(spectra, meta(spectra)$Name), keep)
clean_spec = Reduce(combine, spec.list)
return(clean_spec)
}
################################################################################
#Primary functions
################################################################################
#This function adds the metadata, cuts spectra to wavelength 400:2400, removes
#reflectance values greater than 1, reduces the data to the 4 measurements
#that are closest to the mean for each individual and smooths the spectra.
thebigclean <- function(spectra_path, metadata_path){
meta.spectra = add_meta(spectra_path, metadata_path)
spectra_cut = meta.spectra[, 400:2400]
spec1 = spectra_cut[!rowSums(spectra_cut > 1),]
spec2 = trim.spectra(spec1)
clean_spectra = smooth(spec2)
return(clean_spectra)
}
################################################################################
#Set working directory to folder containing downloaded spectral data
################################################################################
setwd("C:/Users/istas/OneDrive/Documents/Dryas Research/Dryas 2.0")
es_w_path = "Scans_raw/Wet Scans/es_wet"
es_w_meta = "Scans_raw/Wet Scans/es_wet/es_pops_wet.csv"
es_w_clean1 = thebigclean(es_w_path, es_w_meta)
es_w_clean2 = es_w_clean1[!meta(es_w_clean1)$Clone == "Yes",]
es_w_clean = es_w_clean2[!meta(es_w_clean2)$Notes == "not sequenced",]
vn_es_w = normalize(es_w_clean)
#Wickersham Dome A
wda_w_path = "Scans_raw/Wet Scans/wda_wet"
wda_w_meta = "Scans_raw/Wet Scans/wda_wet/wda_pops_wet.csv"
wda_w_clean = thebigclean(wda_w_path, wda_w_meta)
vn_wda_w = normalize(wda_w_clean)
#Wickerhamd Dome B
wdb_w_path = "Scans_raw/Wet Scans/wdb_wet"
wdb_w_meta = "Scans_raw/Wet Scans/wdb_wet/wdb_pops_wet.csv"
wdb_w_clean = thebigclean(wdb_w_path, wdb_w_meta)
vn_wdb_w = normalize(wdb_w_clean)
#all wet
Clean_all_w = Reduce(combine, list(wdb_w_clean, wda_w_clean, es_w_clean))
vn_all_w = normalize(Clean_all_w)
saveRDS(Clean_all_w, "Clean-up/Clean_spectra/clean_all_w.rds")
saveRDS(vn_all_w, "Clean-up/Vector_normalized/vn_all_w.rds")
big3 = readRDS("Clean-up/Vector_normalized/vn_all_w.rds")
names(big3) = meta(big3)$Species
meta(big3)
big3 = readRDS("Clean-up/Vector_normalized/vn_all_w.rds")
names(big3) = meta(big3)$Species
big3.m = as.matrix(big3)
big3.df = as.data.frame(big3)
#Resample by 10 nm
spec_small = resample(big3, seq(400, 2400, by = 10))
spec_mat_s = as.matrix(spec_small)
spec_mat = spec_mat_s
resp = rownames(spec_mat)
rownames(spec_mat) = seq(nrow(spec_mat))
#determine number of components to use
plsda.fit = plsda(spec_mat, resp, ncomp = 30)
perf.plsda = perf(plsda.fit, validation = "Mfold", folds = 5,
progressBar = TRUE, auc = TRUE, nrepeat = 10)
plot(perf.plsda, col = color.mixo(1:3), sd = TRUE,
legend.position = "horizontal")
set.seed(27)
samp <- sample(1:3, nrow(spec_mat), replace = TRUE)
# 1/3 of the data will compose the test set
test <- which(samp == 1)
# rest will compose the training set
train <- setdiff(1:nrow(spec_mat), test)
## For PLS-DA, train the model
plsda.train <- plsda(spec_mat[train, ], resp[train], ncomp = 30)
# then predict
test.predict <- predict(plsda.train, spec_mat[test, ], dist = "max.dist")
# store prediction for the 4th component
prediction <- test.predict$class$max.dist[,24]
# calculate the error rate of the model
confusion.mat = get.confusion_matrix(truth = resp[test], predicted = prediction)
cm1 = as.data.frame(confusion.mat)
get.BER(confusion.mat)
#plot
par(mar = c(2, 4, 3, 4), oma = c(2, 4, 3, 2))
color2D.matplot(cm1,
show.values = TRUE,
axes = FALSE,
xlab = "",
ylab = "",
vcex = 2,
vcol = "black",
extremes = c("white", "deepskyblue3"))
axis(3, at = seq_len(ncol(cm1)) - 0.5,
labels = names(cm1), tick = FALSE, cex.axis = 1)
axis(2, at = seq_len(nrow(cm1)) -0.5,
labels = rev(rownames(cm1)), tick = FALSE, las = 1, cex.axis = 1)
names(big3) = meta(big3)$Species_ID
big3 = readRDS("Clean-up/Vector_normalized/vn_all_w.rds")
names(big3) = meta(big3)$Species_ID
big3.m = as.matrix(big3)
big3.df = as.data.frame(big3)
#Resample by 10 nm
spec_small = resample(big3, seq(400, 2400, by = 10))
spec_mat_s = as.matrix(spec_small)
spec_mat = spec_mat_s
resp = rownames(spec_mat)
rownames(spec_mat) = seq(nrow(spec_mat))
#determine number of components to use
plsda.fit = plsda(spec_mat, resp, ncomp = 30)
set.seed(27)
samp <- sample(1:3, nrow(spec_mat), replace = TRUE)
# 1/3 of the data will compose the test set
test <- which(samp == 1)
# rest will compose the training set
train <- setdiff(1:nrow(spec_mat), test)
## For PLS-DA, train the model
plsda.train <- plsda(spec_mat[train, ], resp[train], ncomp = 30)
# then predict
test.predict <- predict(plsda.train, spec_mat[test, ], dist = "max.dist")
# store prediction for the 4th component
prediction <- test.predict$class$max.dist[,24]
# calculate the error rate of the model
confusion.mat = get.confusion_matrix(truth = resp[test], predicted = prediction)
cm1 = as.data.frame(confusion.mat)
get.BER(confusion.mat)
#plot
par(mar = c(2, 4, 3, 4), oma = c(2, 4, 3, 2))
color2D.matplot(cm1,
show.values = TRUE,
axes = FALSE,
xlab = "",
ylab = "",
vcex = 2,
vcol = "black",
extremes = c("white", "deepskyblue3"))
axis(3, at = seq_len(ncol(cm1)) - 0.5,
labels = names(cm1), tick = FALSE, cex.axis = 1)
axis(2, at = seq_len(nrow(cm1)) -0.5,
labels = rev(rownames(cm1)), tick = FALSE, las = 1, cex.axis = 1)
prediction <- test.predict$class$max.dist[,25]
# calculate the error rate of the model
confusion.mat = get.confusion_matrix(truth = resp[test], predicted = prediction)
cm1 = as.data.frame(confusion.mat)
get.BER(confusion.mat)
#plot
par(mar = c(2, 4, 3, 4), oma = c(2, 4, 3, 2))
color2D.matplot(cm1,
show.values = TRUE,
axes = FALSE,
xlab = "",
ylab = "",
vcex = 2,
vcol = "black",
extremes = c("white", "deepskyblue3"))
axis(3, at = seq_len(ncol(cm1)) - 0.5,
labels = names(cm1), tick = FALSE, cex.axis = 1)
axis(2, at = seq_len(nrow(cm1)) -0.5,
labels = rev(rownames(cm1)), tick = FALSE, las = 1, cex.axis = 1)
prediction <- test.predict$class$max.dist[,27]
# calculate the error rate of the model
confusion.mat = get.confusion_matrix(truth = resp[test], predicted = prediction)
cm1 = as.data.frame(confusion.mat)
get.BER(confusion.mat)
prediction <- test.predict$class$max.dist[,29]
# calculate the error rate of the model
confusion.mat = get.confusion_matrix(truth = resp[test], predicted = prediction)
cm1 = as.data.frame(confusion.mat)
get.BER(confusion.mat)
prediction <- test.predict$class$max.dist[,30]
# calculate the error rate of the model
confusion.mat = get.confusion_matrix(truth = resp[test], predicted = prediction)
cm1 = as.data.frame(confusion.mat)
get.BER(confusion.mat)
prediction <- test.predict$class$max.dist[,28]
# calculate the error rate of the model
confusion.mat = get.confusion_matrix(truth = resp[test], predicted = prediction)
cm1 = as.data.frame(confusion.mat)
get.BER(confusion.mat)
par(mar = c(2, 4, 3, 4), oma = c(2, 4, 3, 2))
color2D.matplot(cm1,
show.values = TRUE,
axes = FALSE,
xlab = "",
ylab = "",
vcex = 2,
vcol = "black",
extremes = c("white", "deepskyblue3"))
axis(3, at = seq_len(ncol(cm1)) - 0.5,
labels = names(cm1), tick = FALSE, cex.axis = 1)
axis(2, at = seq_len(nrow(cm1)) -0.5,
labels = rev(rownames(cm1)), tick = FALSE, las = 1, cex.axis = 1)
prediction <- test.predict$class$max.dist[,15]
# calculate the error rate of the model
confusion.mat = get.confusion_matrix(truth = resp[test], predicted = prediction)
cm1 = as.data.frame(confusion.mat)
get.BER(confusion.mat)
# store prediction for the 4th component
prediction <- test.predict$class$max.dist[,20]
# calculate the error rate of the model
confusion.mat = get.confusion_matrix(truth = resp[test], predicted = prediction)
cm1 = as.data.frame(confusion.mat)
get.BER(confusion.mat)
library(spectrolab)
library(ggplot2)
setwd("C:/Users/istas/OneDrive/Documents/Dryas Research/Dryas 2.0")
spec = readRDS("Clean-up/Vector_normalized/all_vn.rds")
log = read.csv("log.csv")
meta(spec) <- log
data = meta(spec)
plot(data$N.mu)
plot(data$N.mu, main = "Number of leaf layers (N) predicted by PROSPECT", xlab = "N",
ylab = "Sample number")
plot(data$N.mu, main = "Number of leaf layers (N) predicted by PROSPECT", xlab = "Sample number",
ylab = "N")
par(mar = c(2,2,2,2))
plot(data$N.mu, main = "Number of leaf layers (N) predicted by PROSPECT", xlab = "Sample number",
ylab = "N")
par(mar = c(3,3,1,1))
plot(data$N.mu, main = "Number of leaf layers (N) predicted by PROSPECT", xlab = "Sample number",
ylab = "N")
par(mar = c(4,4,2,1))
plot(data$N.mu, main = "Number of leaf layers (N) predicted by PROSPECT", xlab = "Sample number",
ylab = "N")
setwd("C:/Users/istas/OneDrive/Documents/Dryas Research/Dryas 2.0")
setwd("C:/Users/istas/OneDrive/Documents/Dryas Research/Dryas 2.0")
spec_all = readRDS("Clean-up/Vector_normalized/all_vn.rds")
spec_wet = readRDS("Clean-up/Vector_normalized/vn_all_w.rds")
es_dry = spec_all[meta(spec_all)$Location == "Eagle Summit",]
es_wet = spec_wet[meta(spec_wet)$Location == "Eagle Summit",]
meta(es_wet)
es_wet_oct = es_wet[meta(es_wet)$Species_ID == "DO",]
meta(es_wet_oct)
wet = es_wet[meta(es_wet)$Name == "DryoctESA52",]
meta(es_dry)
dry = es_dry[meta(es_dry)$Name == "DryoctESA52",]
dry
wet
par(mfrow=c(1,2), mar=c(4,4,4,2), oma = c(4,3,2,2))
plot(wet, lwd = 0.5, lty = 1, col = "grey50", main="Wet",
cex.lab = 1.2, ylim = c(0, .04), ylab = "Vector Normalized Reflectance", xlab = "Wavelength(nm)")
plot_regions(wet, regions = default_spec_regions(), add = TRUE)
plot(dry, lwd = 0.5, lty = 1, col = "grey50", main="Dry",
cex.lab = 1.2, ylim = c(0, .04), ylab = "Vector Normalized Reflectance", xlab = "Wavelength(nm)")
plot_regions(dry, regions = default_spec_regions(), add = TRUE)
par(mfrow=c(1,2), mar=c(4,4,4,2), oma = c(4,3,2,2))
plot(wet, lwd = 0.5, lty = 1, col = "grey50", main="Wet",
cex.lab = 1.2, ylim = c(0, .04), ylab = "Vector Normalized Reflectance", xlab = "Wavelength(nm)")
plot_regions(wet, regions = default_spec_regions(), add = TRUE)
plot(dry, lwd = 0.5, lty = 1, col = "grey50", main="Dry",
cex.lab = 1.2, ylim = c(0, .04), ylab = NA, xlab = "Wavelength(nm)")
plot_regions(dry, regions = default_spec_regions(), add = TRUE)
#Clean up scans
#Data can be obtained from shared Google Drive folder:
#FM_Polar_Studies > Dryas_Spectral_Analyses > Scans_raw
################################################################################
#Load Packages
################################################################################
library("spectrolab")
################################################################################
#Prerequisite functions
################################################################################
#add metadata to raw spectra
add_meta <- function(spectra_path, metadata_path){
spectra_raw = read_spectra(path = spectra_path, format = "sed")
metadata = read.csv(file = metadata_path, header = TRUE, stringsAsFactors = FALSE)
meta(spectra_raw) <- metadata
return(spectra_raw)
}
#The goal of the following functions is to keep the 4 spectral measurements that
#are closest to the mean reflectance values for each individual plant. The
#sample design involved stacking leaves 3 times and taking 2 reflectance
#measurements per stack interval. Therefore, the first stack tended to have much
#of the black background showing, and the third stack probably had more light
#reflected from the leaves than observed in nature (think about how stacking a
#leaf would simulate scanning a thicker leaf). The idea here is that restricting
#measurements to the 3 closest to the mean should provide more data to work with
#compared to just taking the mean while also removing high and low reflectance
#measurements caused by sample design.
#subtract the mean reflectance from measured reflectance (i.e. calculate
#distance from the mean)
center_scale = function(spectra){
scale(spectra, scale = FALSE)
}
#rank spectra by distance from the mean
dist.rank = function(spectra){
rank(rowSums(abs(center_scale(spectra))))
}
#keep the 4 spectra per individual plant that are closest to the mean
keep = function(spectra){
a = dist.rank(spectra)
x1 = subset(spectra, a < 5)
}
#split the spectra objects to individual plants, apply above funtions, and
#recombine them.
trim.spectra = function(spectra){
spec.list = lapply(split(spectra, meta(spectra)$Name), keep)
clean_spec = Reduce(combine, spec.list)
return(clean_spec)
}
################################################################################
#Primary functions
################################################################################
#This function adds the metadata, cuts spectra to wavelength 400:2400, removes
#reflectance values greater than 1, reduces the data to the 4 measurements
#that are closest to the mean for each individual and smooths the spectra.
thebigclean <- function(spectra_path, metadata_path){
meta.spectra = add_meta(spectra_path, metadata_path)
spectra_cut = meta.spectra[, 400:2400]
spec1 = spectra_cut[!rowSums(spectra_cut > 1),]
spec2 = trim.spectra(spec1)
clean_spectra = smooth(spec2)
return(clean_spectra)
}
setwd("C:/Users/istas/OneDrive/Documents/Dryas Research/Dryas 2.0")
es_path = "Scans_raw/Dry Scans/Eagle_Summit/es-dry"
es_meta = "Scans_raw/Dry Scans/Eagle_Summit/es_pops.csv"
es_raw = add_meta(es_path, es_meta)
es_raw
es_raw = es_raw[, 400:2400]
plot(es_raw)
wdb_path = "Scans_raw/Dry Scans/Wickersham_B/wdb-dry"
wdb_meta = "Scans_raw/Dry Scans/Wickersham_B/wdb_pops.csv"
wdb_raw1 = add_meta(wdb_path, wdb_meta)
wdb_raw = wdb_raw1[, 400:2400]
plot(wdb_raw)
par(mfrow=c(1,2), mar=c(4,4,4,2), oma = c(4,3,2,2))
plot(wdb_raw, lwd = 0.5, lty = 1, col = "grey50", main="WDB Raw Scans",
cex.lab = 1.2, ylab = "Vector Normalized Reflectance", xlab = "Wavelength")
plot_regions(wdb_raw, regions = default_spec_regions(), add = TRUE)
plot(mean(wdb_raw), lwd = 0.5, lty = 1, col = "grey50", main="80% spectral quantile",
cex.lab = 1.2,  xlab = "Wavelength (nm)", ylab = NA)
plot_quantile(wdb_raw, total_prob = 0.8, col = rgb(1, 0, 0, 0.25), border = FALSE, add = TRUE)
plot_regions(wdb_raw, regions = default_spec_regions(), add = TRUE)
par(mfrow=c(1,2), mar=c(4,4,4,2), oma = c(4,3,2,2))
plot(wdb_raw, lwd = 0.5, lty = 1, col = "grey50", main="WDB Raw Scans",
cex.lab = 1.2, ylab = "Vector Normalized Reflectance", xlab = "Wavelength")
plot_regions(wdb_raw, regions = default_spec_regions(), add = TRUE)
plot(mean(wdb_raw), lwd = 0.5, lty = 1, col = "grey50", main="80% spectral quantile",
cex.lab = 1.2,  xlab = "Wavelength (nm)", ylab = NA, ylim = c(0, 2.5))
plot_quantile(wdb_raw, total_prob = 0.8, col = rgb(1, 0, 0, 0.25), border = FALSE, add = TRUE)
plot_regions(wdb_raw, regions = default_spec_regions(), add = TRUE)
par(mfrow=c(1,1), mar=c(4,4,4,2), oma = c(4,3,2,2))
plot(wdb_raw, lwd = 0.5, lty = 1, col = "grey50", main="WDB Raw Scans",
cex.lab = 1.2, ylab = "Vector Normalized Reflectance", xlab = "Wavelength")
plot_regions(wdb_raw, regions = default_spec_regions(), add = TRUE)
par(mfrow=c(1,1), mar=c(4,4,4,2), oma = c(4,3,2,2))
plot(wdb_raw, lwd = 0.5, lty = 1, col = "grey50", main="WDB Raw Scans",
cex.lab = 1.2, ylab = "Reflectance", xlab = "Wavelength")
plot_regions(wdb_raw, regions = default_spec_regions(), add = TRUE)
library(spectrolab)
library(ggplot2)
setwd("C:/Users/istas/OneDrive/Documents/Dryas Research/Dryas 2.0")
spec = readRDS("Clean-up/Vector_normalized/all_vn.rds")
log = read.csv("log.csv")
meta(spec) <- log
data = meta(spec)
plot(data$Cm.mu, type = "l", col = "red", lwd = 1)
lines(data$LMA, col="blue",lty=1, lwd = 1)
plot(sort(data$Cm.mu, decreasing = F), type = "l", col = "red", lwd = 1)
lines(sort(data$LMA, decreasing = F), col="blue",lty=1, lwd = 1)
plot(sort(data$Cm.mu, decreasing = F), type = "l", col = "red", lwd = 1, ylim = c(0, 0.02))
lines(sort(data$LMA, decreasing = F), col="blue",lty=1, lwd = 1)
plot(sort(data$Cm.mu, decreasing = F), type = "l", col = "red", lwd = 1, ylim = c(0, 0.025))
lines(sort(data$LMA, decreasing = F), col="blue",lty=1, lwd = 1)
