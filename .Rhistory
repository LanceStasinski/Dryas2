cm.total = cm.total %>% replace_with_na_all(condition = ~.x == 0)
cm.total = as.matrix(cm.total)
rownames(cm.total) <- c('ES', 'TM', 'WDB', 'BG', 'ES', 'MD',
'TM', 'WDA', 'WDB', 'ES', 'TM', 'WDB')
colnames(cm.total) <- c('ES', 'TM', 'WDB', 'BG', 'ES', 'MD',
'TM', 'WDA', 'WDB', 'ES', 'TM', 'WDB')
#save confusion matrix
write.csv(cm.total, "Figures/cm_final/cm_sp_loc_upsample2_small.csv")
cm.total = read.csv("Figures/cm_final/cm_sp_loc_upsample2_small.csv", stringsAsFactors = T)
cm.total = as.matrix(cm.total)
rownames(cm.total) <- cm.total[,1]
cm.total = cm.total[,-1]
cm.total = mapply(cm.total, FUN = as.numeric)
cm.total = matrix(data = cm.total, ncol = 12, nrow = 12)
rownames(cm.total) <- c('ES', 'TM', 'WDB', 'BG', 'ES', 'TM',
'MD', 'WDA', 'WDB', 'ES', 'TM', 'WDB')
colnames(cm.total) <- c('ES', 'TM', 'WDB', 'BG', 'ES', 'TM',
'MD', 'WDA', 'WDB', 'ES', 'TM', 'WDB')
cols = colorRampPalette(c('#f5f5f5', '#b35806'))
par(mar = c(1,2,1,1), oma = c(1,1,3,1))
corrplot(cm.total,
is.corr = T,
method = 'square',
col = cols(10),
addCoef.col = '#542788',
tl.srt = 0,
tl.offset = 1,
number.digits = 2,
tl.cex = 1.2,
cl.cex = 1,
number.cex = 1.5,
tl.col = 'black',
cl.pos = 'n',
na.label = 'square',
na.label.col = 'white',
addgrid.col = 'grey')
#data
spec_all = readRDS("Clean-up/Clean_spectra/clean_all.rds")
spec_all = spec_all[!meta(spec_all)$sp_loc == "NaN",]
spec_all.m = as.matrix(spec_all)
spec_all.df = as.data.frame(spec_all)
#Resample by every 10 nm
spec_small = resample(spec_all, seq(400, 2400, by = 10))
spec_mat_s = as.matrix(spec_small)
spec_mat = spec_mat_s
#combine relavant meta data to matrix
spec_df = as.data.frame(spec_mat)
spec_df = cbind(spec_df, spec_all.df$sp_loc)
colnames(spec_df)[colnames(spec_df) == "spec_all.df$sp_loc"] <- "sp_loc"
################################################################################
#Run PLSDA
################################################################################
#Set number of components to be used
ncomp = 60
#create vectors, lists, and matrices to store metrics and loadings
accuracy <- c()
kappa <- c()
k.fit <- matrix(nrow = ncomp)
cm.list <- list()
#start of PLSDA code
for(i in 1:100){
#create data partition: 70% of data for training, 30% for testing
inTrain <- caret::createDataPartition(
y = spec_df$sp_loc,
p = .8,
list = FALSE
)
training <- spec_df[inTrain,]
testing <- spec_df[-inTrain,]
#tune model: 10-fold cross-validation repeated 3 times
ctrl <- trainControl(
method = "repeatedcv",
number = 10,
sampling = 'down',
repeats = 3)
#Fit model. Note max iterations set to 10000 to allow model convergence
plsFit <- train(
sp_loc ~ .,
data = training,
maxit = 100000,
method = "pls",
trControl = ctrl,
tuneLength = ncomp)
#kappa objects for determining n components
k = assign(paste0('k', i), as.matrix(plsFit$results$Kappa))
k.fit <- cbind(k.fit, get('k'))
#test model using the testing data partition (30% of data)
plsClasses <- predict(plsFit, newdata = testing)
#confusion/classification matrix objects to assess accuracy
cm = confusionMatrix(data = plsClasses, as.factor(testing$sp_loc))
cm.m = assign(paste0("cm", i), as.matrix(cm))
cm.list <- list.append(cm.list, get('cm.m'))
ac <- assign(paste0('acc',i), cm$overall[1])
accuracy <- append(accuracy, get('ac'))
kap = assign(paste0("kap",i), cm$overall[2])
kappa <- append(kappa, get('kap'))
}
################################################################################
#Kappa and Accuracy assessed after 100 iterations
################################################################################
mean.acc = mean(accuracy)
sd.acc = sd(accuracy)
mean.kap = mean(kappa)
sd.kap = sd(kappa)
mean.acc
sd.acc
mean.kap
sd.kap
################################################################################
#Kappa values for choosing the optimal number of components to use
################################################################################
k.total = k.fit[,-1]
kavg = as.matrix(rowMeans(k.total))
ksd = as.matrix(rowSds(k.total))
klower = kavg - ksd
khigher = kavg + ksd
#Graph to visually choose optimal number of components
x = 1:60
par(mar = c(5.1, 4.1, 4.1, 2.1), oma = c(5.1, 4.1, 4.1, 2.1))
plot(x, kavg, type = 'p', pch = 16, cex = .75, ylab = 'Kappa',
xlab = 'Component', xlim = c(1,60), main = 'Kappa for sp_loc')
arrows(x, klower, x, khigher,length=0.05, angle=90, code=3)
abline(v = 45, col = 'blue')
abline(h = max(klower), col = "Red")
legend('bottomright', legend = c('Mean', 'Maximum kappa','Best component'),
pch = c(16, NA, NA), lty = c(NA, 1, 1), col = c('black', 'red', 'blue'))
x = 1:60
par(mar = c(5.1, 4.1, 4.1, 2.1), oma = c(5.1, 4.1, 4.1, 2.1))
plot(x, kavg, type = 'p', pch = 16, cex = .75, ylab = 'Kappa',
xlab = 'Component', xlim = c(1,60), main = 'Kappa for sp_loc')
arrows(x, klower, x, khigher,length=0.05, angle=90, code=3)
abline(v = max(kavg), col = 'blue')
abline(h = max(klower), col = "Red")
legend('bottomright', legend = c('Mean', 'Maximum kappa','Best component'),
pch = c(16, NA, NA), lty = c(NA, 1, 1), col = c('black', 'red', 'blue'))
max(kavg)
kavg
#Graph to visually choose optimal number of components
x = 1:60
par(mar = c(5.1, 4.1, 4.1, 2.1), oma = c(5.1, 4.1, 4.1, 2.1))
plot(x, kavg, type = 'p', pch = 16, cex = .75, ylab = 'Kappa',
xlab = 'Component', xlim = c(1,60), main = 'Kappa for sp_loc')
arrows(x, klower, x, khigher,length=0.05, angle=90, code=3)
abline(v = 29, col = 'blue')
abline(h = max(klower), col = "Red")
legend('bottomright', legend = c('Mean', 'Maximum kappa','Best component'),
pch = c(16, NA, NA), lty = c(NA, 1, 1), col = c('black', 'red', 'blue'))
#data
spec_all = readRDS("Clean-up/Clean_spectra/clean_all.rds")
spec_all = spec_all[!meta(spec_all)$Species_ID == "NaN",]
spec_all.m = as.matrix(spec_all)
spec_all.df = as.data.frame(spec_all)
#Resample by every 10 nm
spec_small = resample(spec_all, seq(400, 2400, by = 10))
spec_mat_s = as.matrix(spec_small)
spec_mat = spec_mat_s
#combine relavant meta data to matrix
spec_df = as.data.frame(spec_mat)
spec_df = cbind(spec_df, spec_all.df$Species_ID)
colnames(spec_df)[colnames(spec_df) == "spec_all.df$Species_ID"] <- "Species_ID"
################################################################################
#Run PLSDA
################################################################################
#Set number of components to be used
ncomp = 60
#create vectors, lists, and matrices to store metrics and loadings
accuracy <- c()
kappa <- c()
k.fit <- matrix(nrow = ncomp)
cm.list <- list()
#start of PLSDA code
for(i in 1:100){
#create data partition: 70% of data for training, 30% for testing
inTrain <- caret::createDataPartition(
y = spec_df$Species_ID,
p = .8,
list = FALSE
)
training <- spec_df[inTrain,]
testing <- spec_df[-inTrain,]
#tune model: 10-fold cross-validation repeated 3 times
ctrl <- trainControl(
method = "repeatedcv",
number = 10,
sampling = 'down',
repeats = 3)
#Fit model. Note max iterations set to 10000 to allow model convergence
plsFit <- train(
Species_ID ~ .,
data = training,
maxit = 100000,
method = "pls",
trControl = ctrl,
tuneLength = ncomp)
#kappa objects for determining n components
k = assign(paste0('k', i), as.matrix(plsFit$results$Kappa))
k.fit <- cbind(k.fit, get('k'))
#test model using the testing data partition (30% of data)
plsClasses <- predict(plsFit, newdata = testing)
#confusion/classification matrix objects to assess accuracy
cm = confusionMatrix(data = plsClasses, as.factor(testing$Species_ID))
cm.m = assign(paste0("cm", i), as.matrix(cm))
cm.list <- list.append(cm.list, get('cm.m'))
ac <- assign(paste0('acc',i), cm$overall[1])
accuracy <- append(accuracy, get('ac'))
kap = assign(paste0("kap",i), cm$overall[2])
kappa <- append(kappa, get('kap'))
}
################################################################################
#Kappa and Accuracy assessed after 100 iterations
################################################################################
mean.acc = mean(accuracy)
sd.acc = sd(accuracy)
mean.kap = mean(kappa)
sd.kap = sd(kappa)
mean.acc
sd.acc
mean.kap
sd.kap
################################################################################
#Kappa values for choosing the optimal number of components to use
################################################################################
k.total = k.fit[,-1]
kavg = as.matrix(rowMeans(k.total))
ksd = as.matrix(rowSds(k.total))
klower = kavg - ksd
khigher = kavg + ksd
#Graph to visually choose optimal number of components
x = 1:60
par(mar = c(5.1, 4.1, 4.1, 2.1), oma = c(5.1, 4.1, 4.1, 2.1))
plot(x, kavg, type = 'p', pch = 16, cex = .75, ylab = 'Kappa',
xlab = 'Component', xlim = c(1,60), main = 'Kappa for Species_ID')
arrows(x, klower, x, khigher,length=0.05, angle=90, code=3)
abline(v = 29, col = 'blue')
abline(h = max(klower), col = "Red")
legend('bottomright', legend = c('Mean', 'Maximum kappa','Best component'),
pch = c(16, NA, NA), lty = c(NA, 1, 1), col = c('black', 'red', 'blue'))
max(kavg)
kavg
library(spectrolab)
library(caret)
library(dplyr)
library(mlbench)
library(corrplot)
library(matrixStats)
library(naniar)
library(rlist)
setwd("C:/Users/istas/OneDrive/Documents/Dryas Research/Dryas 2.0")
################################################################################
#Data setup !!!!NOTE: use ctrl+f to find a replace the field to be classified!!!
################################################################################
#data
spec_all = readRDS("Clean-up/Clean_spectra/clean_all.rds")
spec_all = spec_all[!meta(spec_all)$sp_loc == "NaN",]
spec_all.m = as.matrix(spec_all)
spec_all.df = as.data.frame(spec_all)
#Resample by every 10 nm
spec_small = resample(spec_all, seq(400, 2400, by = 10))
spec_mat_s = as.matrix(spec_small)
spec_mat = spec_mat_s
#combine relavant meta data to matrix
spec_df = as.data.frame(spec_mat)
spec_df = cbind(spec_df, spec_all.df$sp_loc)
colnames(spec_df)[colnames(spec_df) == "spec_all.df$sp_loc"] <- "sp_loc"
################################################################################
#Run PLSDA
################################################################################
#Set number of components to be used
ncomp = 29
#create vectors, lists, and matrices to store metrics and loadings
accuracy <- c()
kappa <- c()
k.fit <- matrix(nrow = ncomp)
cm.list <- list()
#start of PLSDA code
for(i in 1:100){
#create data partition: 70% of data for training, 30% for testing
inTrain <- caret::createDataPartition(
y = spec_df$sp_loc,
p = .8,
list = FALSE
)
training <- spec_df[inTrain,]
testing <- spec_df[-inTrain,]
#tune model: 10-fold cross-validation repeated 3 times
ctrl <- trainControl(
method = "repeatedcv",
number = 10,
sampling = 'up',
repeats = 3)
#Fit model. Note max iterations set to 10000 to allow model convergence
plsFit <- train(
sp_loc ~ .,
data = training,
maxit = 100000,
method = "pls",
trControl = ctrl,
tuneLength = ncomp)
#kappa objects for determining n components
k = assign(paste0('k', i), as.matrix(plsFit$results$Kappa))
k.fit <- cbind(k.fit, get('k'))
#test model using the testing data partition (30% of data)
plsClasses <- predict(plsFit, newdata = testing)
#confusion/classification matrix objects to assess accuracy
cm = confusionMatrix(data = plsClasses, as.factor(testing$sp_loc))
cm.m = assign(paste0("cm", i), as.matrix(cm))
cm.list <- list.append(cm.list, get('cm.m'))
ac <- assign(paste0('acc',i), cm$overall[1])
accuracy <- append(accuracy, get('ac'))
kap = assign(paste0("kap",i), cm$overall[2])
kappa <- append(kappa, get('kap'))
}
mean.acc = mean(accuracy)
sd.acc = sd(accuracy)
mean.kap = mean(kappa)
sd.kap = sd(kappa)
mean.acc
sd.acc
mean.kap
sd.kap
cm.avg = Reduce('+', cm.list)/100
cm.avg = t(cm.avg)
cm.total = cm.avg/rowSums(cm.avg)
#standard deviations
f1 <- function(lst){
n <- length(lst);
rc <- dim(lst[[1]]);
ar1 <- array(unlist(lst), c(rc, n));
round(apply(ar1, c(1, 2), sd), 2);
}
cm.sd = f1(cm.list)
cm.sd = t(cm.sd)
cm.sd = cm.sd/rowSums(cm.avg)
rownames(cm.sd) <- c('ES', 'TM', 'WDB', 'BG', 'ES', 'MD',
'TM', 'WDA', 'WDB', 'ES', 'TM', 'WDB')
colnames(cm.sd) <- c('ES', 'TM', 'WDB', 'BG', 'ES', 'MD',
'TM', 'WDA', 'WDB', 'ES', 'TM', 'WDB')
write.csv(cm.sd, file = 'Figures/cm_final/standard deviations/sp_loc_sd_upsample2_small2.csv')
cm.total = as.data.frame(cm.total)
cm.total = cm.total %>% replace_with_na_all(condition = ~.x == 0)
cm.total = as.matrix(cm.total)
rownames(cm.total) <- c('ES', 'TM', 'WDB', 'BG', 'ES', 'MD',
'TM', 'WDA', 'WDB', 'ES', 'TM', 'WDB')
colnames(cm.total) <- c('ES', 'TM', 'WDB', 'BG', 'ES', 'MD',
'TM', 'WDA', 'WDB', 'ES', 'TM', 'WDB')
#save confusion matrix
write.csv(cm.total, "Figures/cm_final/cm_sp_loc_upsample2_small2.csv")
cm.total = read.csv("Figures/cm_final/cm_sp_loc_upsample2_small2.csv", stringsAsFactors = T)
cm.total = as.matrix(cm.total)
rownames(cm.total) <- cm.total[,1]
cm.total = cm.total[,-1]
cm.total = mapply(cm.total, FUN = as.numeric)
cm.total = matrix(data = cm.total, ncol = 12, nrow = 12)
rownames(cm.total) <- c('ES', 'TM', 'WDB', 'BG', 'ES', 'TM',
'MD', 'WDA', 'WDB', 'ES', 'TM', 'WDB')
colnames(cm.total) <- c('ES', 'TM', 'WDB', 'BG', 'ES', 'TM',
'MD', 'WDA', 'WDB', 'ES', 'TM', 'WDB')
#plot confusion matrix
cols = colorRampPalette(c('#f5f5f5', '#b35806'))
par(mar = c(1,2,1,1), oma = c(1,1,3,1))
corrplot(cm.total,
is.corr = T,
method = 'square',
col = cols(10),
addCoef.col = '#542788',
tl.srt = 0,
tl.offset = 1,
number.digits = 2,
tl.cex = 1.2,
cl.cex = 1,
number.cex = 1.5,
tl.col = 'black',
cl.pos = 'n',
na.label = 'square',
na.label.col = 'white',
addgrid.col = 'grey')
#data
spec_all = readRDS("Clean-up/Clean_spectra/clean_all.rds")
spec_all = spec_all[!meta(spec_all)$Species_ID == "NaN",]
spec_all.m = as.matrix(spec_all)
spec_all.df = as.data.frame(spec_all)
#Resample by every 10 nm
spec_small = resample(spec_all, seq(400, 2400, by = 10))
spec_mat_s = as.matrix(spec_small)
spec_mat = spec_mat_s
#combine relavant meta data to matrix
spec_df = as.data.frame(spec_mat)
spec_df = cbind(spec_df, spec_all.df$Species_ID)
colnames(spec_df)[colnames(spec_df) == "spec_all.df$Species_ID"] <- "Species_ID"
################################################################################
#Run PLSDA
################################################################################
#Set number of components to be used
ncomp = 22
#create vectors, lists, and matrices to store metrics and loadings
accuracy <- c()
kappa <- c()
k.fit <- matrix(nrow = ncomp)
cm.list <- list()
#start of PLSDA code
for(i in 1:100){
#create data partition: 70% of data for training, 30% for testing
inTrain <- caret::createDataPartition(
y = spec_df$Species_ID,
p = .8,
list = FALSE
)
training <- spec_df[inTrain,]
testing <- spec_df[-inTrain,]
#tune model: 10-fold cross-validation repeated 3 times
ctrl <- trainControl(
method = "repeatedcv",
number = 10,
sampling = 'up',
repeats = 3)
#Fit model. Note max iterations set to 10000 to allow model convergence
plsFit <- train(
Species_ID ~ .,
data = training,
maxit = 100000,
method = "pls",
trControl = ctrl,
tuneLength = ncomp)
#kappa objects for determining n components
k = assign(paste0('k', i), as.matrix(plsFit$results$Kappa))
k.fit <- cbind(k.fit, get('k'))
#test model using the testing data partition (30% of data)
plsClasses <- predict(plsFit, newdata = testing)
#confusion/classification matrix objects to assess accuracy
cm = confusionMatrix(data = plsClasses, as.factor(testing$Species_ID))
cm.m = assign(paste0("cm", i), as.matrix(cm))
cm.list <- list.append(cm.list, get('cm.m'))
ac <- assign(paste0('acc',i), cm$overall[1])
accuracy <- append(accuracy, get('ac'))
kap = assign(paste0("kap",i), cm$overall[2])
kappa <- append(kappa, get('kap'))
}
mean.acc = mean(accuracy)
sd.acc = sd(accuracy)
mean.kap = mean(kappa)
sd.kap = sd(kappa)
mean.acc
sd.acc
mean.kap
sd.kap
#take average of 100 confusion matrices, reorient matrix, change averages to
#proportions
cm.avg = Reduce('+', cm.list)/100
cm.avg = t(cm.avg)
cm.total = cm.avg/rowSums(cm.avg)
#standard deviations
f1 <- function(lst){
n <- length(lst);
rc <- dim(lst[[1]]);
ar1 <- array(unlist(lst), c(rc, n));
round(apply(ar1, c(1, 2), sd), 2);
}
cm.sd = f1(cm.list)
cm.sd = t(cm.sd)
cm.sd = cm.sd/rowSums(cm.avg)
rownames(cm.sd) <- c('DA', 'DO', 'DX')
colnames(cm.sd) <- c('DA', 'DO', 'DX')
write.csv(cm.sd, file = 'Figures/cm_final/standard deviations/Species_ID_sd_upsample2_small2.csv')
#format matrix for plotting
cm.total = as.data.frame(cm.total)
cm.total = cm.total %>% replace_with_na_all(condition = ~.x == 0)
cm.total = as.matrix(cm.total)
rownames(cm.total) <- c('DA', 'DO', 'DX')
colnames(cm.total) <- c('DA', 'DO', 'DX')
#save confusion matrix
write.csv(cm.total, "Figures/cm_final/cm_Species_ID_upsample2_small2.csv")
cols = colorRampPalette(c('#f5f5f5', '#b35806'))
par(mar = c(1,2,1,1), oma = c(1,1,3,1))
corrplot(cm.total,
is.corr = T,
method = 'square',
col = cols(10),
addCoef.col = '#542788',
tl.srt = 0,
tl.offset = 1,
number.digits = 2,
tl.cex = 1.2,
cl.cex = 1,
number.cex = 1.5,
tl.col = 'black',
cl.pos = 'n',
na.label = 'square',
na.label.col = 'white',
addgrid.col = 'grey')
mtext("Reference", side = 2, line = -8, cex = 2.5)
mtext("Prediction", side = 3, cex = 2.5, at = 2, line = 3)
par(mar = c(1,2,2,1), oma = c(1,1,3,1))
corrplot(cm.total,
is.corr = T,
method = 'square',
col = cols(10),
addCoef.col = '#542788',
tl.srt = 0,
tl.offset = 1,
number.digits = 2,
tl.cex = 1.2,
cl.cex = 1,
number.cex = 1.5,
tl.col = 'black',
cl.pos = 'n',
na.label = 'square',
na.label.col = 'white',
addgrid.col = 'grey')
mtext("Reference", side = 2, line = -8, cex = 2.5)
mtext("Prediction", side = 3, cex = 2.5, at = 2, line = 3)
hyb = spec_all[meta(spec_all)$Species_ID == 'DX',]
meta(hyb)
